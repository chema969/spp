% !TeX spellcheck = en_GB

%% bare_jrnl.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% see http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



\documentclass[journal]{IEEEtran}
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[journal]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.




% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
\usepackage[pdftex]{graphicx}
% declare the path(s) where your graphic files are
% \graphicspath{{../pdf/}{../jpeg/}}
% and their extensions so you won't have to specify these with
% every instance of \includegraphics
% \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
% or other class option (dvipsone, dvipdf, if not using dvips). graphicx
% will default to the driver specified in the system graphics.cfg if no
% driver is specified.
% \usepackage[dvips]{graphicx}
% declare the path(s) where your graphic files are
% \graphicspath{{../eps/}}
% and their extensions so you won't have to specify these with
% every instance of \includegraphics
% \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
\usepackage{amsmath}
\usepackage{mathrsfs}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix




%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and 
% Axel Sommerfeldt. This package may be useful when used in conjunction with 
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/pkg/endfloat
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a 
% page by themselves.




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.

\usepackage[utf8]{inputenc}

%\usepackage[backend=bibtex, style=ieee]{biblatex}

\usepackage{lmodern}

\usepackage{dsfont}
\usepackage{amssymb}

\usepackage{color}

%\usepackage{ulem}

\usepackage{array,booktabs}

\usepackage{hyperref}

% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor mi-ni-mi-zes des-cri-bed di-ffe-rent sig-ni-fi-can-ce}

%\addbibresource{main.bib}

%\renewcommand{\bibfont}{\footnotesize}

\begin{document}
	%
	% paper title
	% Titles are generally capitalized except for words such as a, an, and, as,
	% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
	% not capitalized unless they are the first or last word of the title.
	% Linebreaks \\ can be used within to get better formatting as desired.
	% Do not put math or special symbols in the title.
	\title{Classification of ordinal data in deep learning: an experimental study}
	%
	%
	% author names and IEEE memberships
	% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
	% a structure at a ~ so this keeps an author's name from being broken across
	% two lines.
	% use \thanks{} to gain access to the first footnote area
	% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
	% was not built to handle multiple paragraphs
	%
	
	\author{Víctor-Manuel~Vargas, Pedro-Antonio~Gutiérrez and César~Hervás}
	
	% note the % following the last \IEEEmembership and also \thanks - 
	% these prevent an unwanted space from occurring between the last author name
	% and the end of the author line. i.e., if you had this:
	% 
	% \author{....lastname \thanks{...} \thanks{...} }
	%                     ^------------^------------^----Do not want these spaces!
	%
	% a space would be appended to the last name and could cause every name on that
	% line to be shifted left slightly. This is one of those "LaTeX things". For
	% instance, "\textbf{A} \textbf{B}" will typeset as "A B" not "AB". To get
	% "AB" then you have to do: "\textbf{A}\textbf{B}"
	% \thanks is no different in this regard, so shield the last } of each \thanks
	% that ends a line with a % and do not let a space in before the next \thanks.
	% Spaces after \IEEEmembership other than the last one are OK (and needed) as
	% you are supposed to have spaces between the names. For what it is worth,
	% this is a minor point as most people would not even notice if the said evil
	% space somehow managed to creep in.
	
	
	
	% The paper headers
	\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2015}%
	{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for IEEE Journals}
	% The only time the second header will appear is for the odd numbered pages
	% after the title page when using the twoside option.
	% 
	% *** Note that you probably will NOT want to include the author's ***
	% *** name in the headers of peer review papers.                   ***
	% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
	% you desire.
	
	
	
	
	% If you want to put a publisher's ID mark on the page you can do it like
	% this:
	%\IEEEpubid{0000--0000/00\$00.00~\copyright~2015 IEEE}
	% Remember, if you use this you must call \IEEEpubidadjcol in the second
	% column for its text to clear the IEEEpubid mark.
	
	
	
	% use for special paper notices
	%\IEEEspecialpapernotice{(Invited Paper)}
	
	
	
	
	% make the title area
	\maketitle
	
	% As a general rule, do not put math, special symbols or citations
	% in the abstract or keywords.
	\begin{abstract}
		This paper proposes a deep convolutional neural network model for ordinal regression based on the use of a family of probabilistic ordinal link functions in the output layer called Cumulative Link Models. This kind of models projects each pattern into a 1-dimensional space. In this case, the projections are estimated by a non-linear deep neural network. A set of ordered thresholds splits this space into the different classes of the problem. Different link functions will be studied in this experimental study, and the results will be contrasted with statistical analysis. To further improve the results, we combine these ordinal models with a loss function that takes into account the distance between the categories, based on the weighted Kappa index. The experiments run over two different ordinal classification problems, and the statistical tests confirm that these models improve the results of a nominal model and outperform the proposals considered in the literature.
	\end{abstract}
	
	% Note that keywords are not normally used for peerreview papers.
	\begin{IEEEkeywords}
		Deep learning, Ordinal Regression, Link models.
	\end{IEEEkeywords}
	
	
	
	
	
	
	% For peer review papers, you can put extra information on the cover
	% page as needed:
	% \ifCLASSOPTIONpeerreview
	% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
	% \fi
	%
	% For peerreview papers, this IEEEtran command inserts a page break and
	% creates the second title. It will be ignored for other modes.
	\IEEEpeerreviewmaketitle
	
	
	
	\section{Introduction}
	\label{sect:introduction}
	% The very first letter is a 2 line initial drop letter followed
	% by the rest of the first word in caps.
	% 
	% form to use if the first word consists of a single letter:
	% \IEEEPARstart{A}{demo} file is ....
	% 
	% form to use if you need the single drop letter followed by
	% normal text (unknown if ever used by the IEEE):
	% \IEEEPARstart{A}{}demo file is ....
	% 
	% Some journals put the first two words in caps:
	% \IEEEPARstart{T}{his demo} file is ....
	% 
	% Here we have the typical use of a "T" for an initial drop letter
	% and "HIS" in caps to complete the first word.
	\IEEEPARstart{D}{eep learning}, introduced by Yann Lecun~\cite{lecun2015deep}, combines multiple machine learning techniques and allows computational models that are composed of numerous processing layers to learn representations of data with various levels of abstraction. These methods have dramatically improved the state-of-the-art in many domains, such as image classification~\cite{cirecsan2012multi,he2016deep,krizhevsky2012imagenet}, speech recognition~\cite{hinton2012deep}, control problems~\cite{mnih2015human}, object detection~\cite{jiang2016speed,girshick2014rich}, privacy protection~\cite{yu2017iprivacy}, recovery of human pose~\cite{hong2015multimodal}, semantic segmentation~\cite{long2015fully} and image retrieval~\cite{li2015weakly}. Convolutional Neural Networks (CNN) are one of the types of deep networks that are designed to process data that comes in the form of multiple arrays. CNNs are appropriate for images, video, speech and audio processing, and they have been used extensively in the last years for automatic classification tasks~\cite{dong2014learning,sun2013deep,ronneberger2015u}. On image classification tasks, each colour channel is represented by a 2D array. In this case, convolutional layers extract the main features from the pixels of the images and, after that, a fully connected layer classify every sample based on its extracted features. At the output of the CNN, a softmax function provides the probabilities of the set of classes predefined in the model for classification tasks. These outputs are compared against the correct values.
	
	Ordinal classification problems are those classification tasks where labels are ordered, and there are different inter-classes importances for each pair of classes. This kind of problem can be treated as a nominal classification problem, but this discards ordinal information. A better approach is to use specific methods that take into account this kind of information to improve the performance of the classification model. The Proportional Odds Model (POM)~\cite{agresti2010analysis} is an ordinal alternative to the softmax function that was designed for ordinal regression. It belongs to a wider family of models called Cumulative Link Models (CLM). It is inspired in the concept of a latent variable that is projected in an n-dimensional space and a set of thresholds that divides this space into the different ordinal levels. This kind of models uses a link function which can be of different types. The most common link function is the \texttt{logit} one, that is used in POM. However, there are other functions that can be explored. This model will be described in depth in Section \ref{sect:ordinalproblem}.
	
	Ordinal models can also be applied to deep learning models. In the case of convolutional networks, the model projection used by the threshold model will be obtained from the last layer of the network. When working with a 1-dimensional space, the last layer will have only one neuron, and its value will be used to classify every sample in the corresponding class according to the thresholds. Some previous works have used the POM in traditional neural networks~\cite{gutierrez2016ordinal}, but it has not been applied to convolutional networks yet. Also, there are some link functions that has not been explored.
	
	In this paper, an experimental study regarding link functions for Cumulative Link Models will be made. Also, other parameters that can affect the training process and the model performance, like the learning rate of the optimization algorithm and the batch size, and its interaction will be studied. The nominal version of this model will be used as a baseline for comparison. We will contrast the results obtained with statistical analysis to provide more robust conclusions. An approximated ANOVA III \cite{miller1997beyond} test followed by a posthoc Tukey's test \cite{tukey1949comparing} will be performed because of the limitations of the computational time required to run a higher number of executions.
	
	The experiments will be run using two different ordinal datasets: Diabetic Retinopathy~\cite{de2018weighted}, which contains high-resolution fundus images related with diabetes disease, and Adience~\cite{beckham2017unimodal}, which includes human faces images associated with an age range.
	
	This paper is organized as follows: in Section~\ref{sect:relatedwork}, we take a look at previous works related to this paper. Section~\ref{sect:ordinalproblem} present a formal description of an ordinal problem and the Cumulative Link Models. In Section~\ref{sect:experiments}, we describe the model, the experiments and the datasets used, while, in Section~\ref{sect:results}, we present the results obtained and the statistical analysis. Finally, Section~\ref{sect:conclusions} exposes the conclusions of this work.
	
	\section{Related works}
	\label{sect:relatedwork}
	There are many works related to the application and development of CNN models. However, few works are focused on ordinal classification problems.
	
	J. de la Torre et al.~\cite{de2018weighted} proposed the use of a continuous version of the QWK loss function for the optimization algorithm. They compared this cost function against the traditional log-loss function across three different databases, including the Diabetic Retinopathy database as the most complex one. They proved that their function could improve the results as it reduces overfitting and training time. Also, they checked the importance of hyper-parameter tuning. First, they defined QWK metric as follows:
	\begin{equation}
	\text{QWK} = 1 - \frac{\sum\limits_{i,j} \omega_{i,j} O_{i,j}}{\sum\limits_{i,j} \omega_{i,j} E_{i,j}},
	\end{equation}
	where $\omega$ is the penalization matrix (in this case, quadratic weights are considered), $O$ is the confusion matrix and $E$ is the normalized outer product between the prediction and the true vector.
	
	Then, they provided a continuous version of the QWK metric ($\text{QWK}_c$) based on the probabilities of the predictions:
	\begin{equation}
	\text{QWK}_c = 1 - \frac{\sum\limits_{k=1}^N \sum\limits_{q=1}^Q \omega_{t_k, q} P(y = \mathcal{C}_q | \mathbf{x}_k)}{\sum\limits_{i=1}^Q \frac{N_i}{N} \sum\limits_{j=1}^Q ( \omega_{i,j} \sum\limits_{k=1}^N P(y = \mathcal{C}_j | \mathbf{x}_k))},
	\end{equation}
	where $\text{QWK}_c \in [-1,1]$, $\mathbf{x}_k$ and $t_k$ are the input data and the real class of the $k$-th sample, $Q$ is the number of classes, $N$ is the number of samples, $N_i$ is the number of samples of the $i$th class, $P(y = \mathcal{C}_q | \mathbf{x}_k)$ is the probability that the $k$th sample belongs to class $\mathcal{C}_q$ and $\omega_{i,j}$ are the elements of the penalization matrix. Generally, $\omega_{i,j} = \frac{|i-j|^n}{(C-1)^n}$, where $\omega_{i,j} \in [0,1]$.
	
	Then, they defined the loss function based on the QWK as follows:
	\begin{equation}
	\mathscr{L} = \log(1 - \text{QWK}_c),
	\end{equation}
	where $\mathscr{L} \in [-\infty, \log 2]$. This loss is a function to be minimized while the QWK metric must be maximized.
	
	Z. Niu et al. \cite{niu2016ordinal} proposed a learning approach to address ordinal regression problems using convolutional neural networks. They divided the problem into a series of binary classification sub-problems and proposed a multiple output CNN optimization algorithm to collectively resolve these classification sub-problems, taking into account the correlation between them.
	
	C. Beckham and C. Pal~\cite{beckham2017unimodal} proposed a straightforward technique to constrain discrete ordinal probability distributions to be unimodal, via the use of the Poisson and binomial probability distributions. They evaluated this approach in the context of deep learning on two large ordinal image datasets, including the Adience dataset used in this paper, obtaining promising results.	Also, they proposed a simple squared-error reformulation \cite{beckham2016simple} that was sensitive to class ordering.
	
	Adience dataset has been used in other works for human age estimation. E. Eidinger \cite{eidinger2014age} presented an approach using support vector machines and neural networks. J.-C. Chen \cite{chen2016cascaded} proposed a coarse-to-fine strategy for deep convolutional networks. G. Levi \cite{levi2015age} presented another convolutional network model for age estimation. \textcolor{red}{Estos son ordinales?} \textcolor{blue}{No son ordinales pero los cito aqui porque luego los uso para comparar con ellos ya que son los que he encontrado que utilicen las base de datos Adience para predecir edad.}
	
	H. Li et al. \cite{li2017deep} applied deep learning techniques for solving the ordinal problem of Alzheimer's diagnosis and detecting the different levels of the disease.
	
	Y. Liu et al. \cite{liu2017deep} proposed a new approach of which transforms the ordinal regression problem to binary classification problems and use triplets with instances from different categories to train deep neural networks. In this way, high-level features describing the ordinal relationship are extracted automatically.
	
	A. Rios et al. \cite{rios2017ordinal} presented a CNN model designed to handle ordinal regression tasks on psychiatric notes. They combined an ordinal loss function, a CNN model and conventional feature engineering. Also, they applied a technique called Locally Interpretable Model-agnostic Explanation (LIME) to make the non-linear model more interpretable.
	
	S. Chen et al. \cite{chen2017using} proposed a deep method termed Ranking-CNN. This method combines multiple binary CNNs that are trained with ordinal age labels. The binary outputs are aggregated for the final age prediction and they achieved a tighter error bound for ranking-based age estimation.
	
	H. Fu et al. \cite{fu2018deep} applied deep learning techniques to Monocular Depth Estimation. They introduced a spacing-increasing discretization strategy to treat the problem as an ordinal regression problem. They improved the performance when training the network with an ordinary regression loss. Also, they used a multi-scale network structure that avoids unnecessary spatial pooling.
	
	Y. Liu et al. \cite{liu2018constrained} proposed a constrained optimization formulation for the ordinal regression problem which minimizes the negative loglikelihood for multiple categories constrained by the order relationship between instances.
	
	A. Pal et al. \cite{pal2018severity} defined a loss function for CNN that is based on the Earth Mover's Distance and takes into account the ordinal class relationships.
	
	M. ALALI et al. \cite{alali2018multi} proposed a complex CNN architecture for solving Twitter Sentiment Classification as an ordinal problem. They checked that using average pooling preserves significant features that provide more expresiveness to ordinal scale.
	
	\section{Cumulative Link Models (CLM)}
	\label{sect:ordinalproblem}
	An ordinal classification problem consists of predicting the label $y$ of an input vector $\mathbf{x}$, where $\mathbf{x} \in \mathcal{X} \subseteq \mathds{R}^K$ and $y \in \mathcal{Y}~=~\{\mathcal{C}_1, \mathcal{C}_2, ..., \mathcal{C}_Q\}$, i.e. $\mathbf{x}$ is in a K-dimensional input space, and $y$ is in a label space of $Q$ different labels. The objective of the ordinal problem is to find a function $r : \mathcal{X} \rightarrow \mathcal{Y}$ to predict the labels or categories of new patterns, given a training set of $N$ points, $D = \{(\mathbf{x}_i, y_i), i = 1, ..., N\}$. Labels have a natural ordering in ordinal problems: $\mathcal{C}_1 \prec \mathcal{C}_2 \prec ... \prec \mathcal{C}_Q$. The order between labels makes it possible to compare two different elements of $\mathcal{Y}$ by using the relation $\prec$. This is not possible under the nominal classification setting. In regression (where $y \in \mathds{R}$), real values in $\mathds{R}$ can be ordered by the standard $<$ operator, but labels in ordinal regression ($y \in \mathcal{Y}$) do not carry metric information, so the category serves as a qualitative indication of the pattern rather than a quantitative one.
	
	The Proportional Odds Model (POM) arises from a statistical background and is one of the first models designed explicitly for ordinal regression~\cite{mccullagh1980regression}. It dated back to 1980 and is a member of a wider family of models recognised as Cumulative Link Models (CLM)~\cite{agresti2010analysis}. CLMs predict probabilities of groups of contiguous categories, taking the ordinal scale into account. In this way, cumulative probabilities $P(y \prec C_q |\mathbf{x})$ are estimated, which can be directly related to standard probabilities:
	\begin{align}
		P(y \preceq \mathcal{C}_q | \mathbf{x}) &= P(y = \mathcal{C}_1 | \mathbf{x}) + ... + P(y = \mathcal{C}_q | \mathbf{x}),\\
		P(y = \mathcal{C}_q | \mathbf{x}) &= P(y \preceq \mathcal{C}_q | \mathbf{x}) - P(y \preceq \mathcal{C}_{q-1} | \mathbf{x}),
	\end{align}
	with $q = 2, ..., Q-1$, and considering that $P(y = \mathcal{C}_1 | \mathbf{x}) = P(y \preceq \mathcal{C}_1 | \mathbf{x})$ and $P(y \preceq \mathcal{C}_Q | \mathbf{x}) = 1$.
	
	The model is inspired by the notion of a latent variable, where $f(\textbf{x})$ represents a one-dimensional mapping obtained from the output of the last layer, which has only one neuron. The decision rule $r: \mathcal{X} \rightarrow \mathcal{Y}$ is not fitted directly, but stochastic ordering of space $\mathcal{X}$ is satisfied by the following general model form \cite{herbrich2000large}:
	\begin{equation}
		g^{-1}(P(y \preceq \mathcal{C}_q | \mathbf{x})) = b_q - f(\mathbf{x}), \quad q = 1, ..., Q-1,
	\end{equation}
	where $g^{-1} : [0,1] \rightarrow (-\infty, +\infty)$ is a monotonic function often termed as the inverse link function, and $b_q$ is the threshold defined for class $\mathcal{C}_q$. Consider the latent variable $y^* = f(\mathbf{x})^* = f(\mathbf{x}) + \epsilon$, where $\epsilon$ is the random variable of the error. The most common choice for the probability distribution of $\epsilon$ is the logistic function (which is the default function for POM). Label $\mathcal{C}_q$ is predicted if and only if $f(\mathbf{x}) \in [b_{q-1}, b_q]$, where the function $f$ and $\mathbf{b} = (b_0, b_1, ..., b_{Q-1}, b_Q)$ are to be determined from the data. It is assumed that $b_0 = -\infty$ and $b_Q = +\infty$, so the real line defined by $f(\textbf{x}), \textbf{x} \in \mathcal{X}$, is divided into $Q$ consecutive intervals. Each interval corresponds to a category. The constraints $b_1 \le b_2 \le ... \le b_{Q-1}$ ensures that $P(y \preceq \mathcal{C}_q | \mathbf{x})$ increases with $q$ \cite{mccullagh1980regression}.
	This order is achieved by defining the first threshold and calculating the rest of thresholds from the first in the following form:
	
	\begin{equation}
	b_n = b_1 + \sum_{i=1}^{n-1} \alpha_i^2, \quad n = 2, ..., N,
	\end{equation}
	where $b_1$ and $\alpha_i$ are learnable parameters, and $N$ is the number of classes.
	
	In this work, we use this ordinal model with different link functions for the probability distribution of $\epsilon$, including \texttt{logit}, \texttt{probit} and complementary log-log (\texttt{clog-log}).
	
	These three types of links are explained below and represented in Figure \ref{fig:linkfunctions}. They all follow the same form $P(y \preceq \mathcal{C}_q | \mathbf{x}) = \Phi(b_q - f(\mathbf{x}))$ for a continuous cdf~$\Phi$.
			
	\begin{figure}[!t]
		\centering
		\includegraphics[width=3in]{img/linkfunctions.pdf}
		\caption{Representation of link functions.}
		\label{fig:linkfunctions}
	\end{figure}

	\begin{itemize}
		\item \texttt{Logit}. \texttt{logit} link function is the function used for the Proportional Odds Model. The \texttt{logit} link is:
		\begin{equation}
		\begin{aligned}
		\text{logit}[P(y \preceq \mathcal{C}_q | \mathbf{x})] = \log\frac{P(y \preceq \mathcal{C}_q | \mathbf{x})}{1 - P(y \preceq \mathcal{C}_q | \mathbf{x})}=& \\ = b_q - f(\mathbf{x}), \quad q = 1, ..., Q-1,
		\end{aligned}
		\label{eq:logit}
		\end{equation}		
		or the equivalent expression:		
		\begin{equation}
		P(y \preceq \mathcal{C}_q | \mathbf{x}) = \frac{1}{1 + e^{-(b - f(\mathbf{x}))}}.
		\label{eq:logit2}
		\end{equation}
		
		\item \texttt{Probit}. \texttt{probit} link function is the inverse of the standard normal cumulative distribution function (cdf) $\Phi$. Its expression is:
		\begin{equation}
		\begin{aligned}
		\Phi^{-1}[P(y \preceq \mathcal{C}_q | \mathbf{x})] = b_q - f(\mathbf{x}), \quad &q = 1, ..., Q-1,\\
		P(y \preceq \mathcal{C}_q | \mathbf{x}) = \Phi(b_q - f(\mathbf{x})), \quad &q = 1, ..., Q-1,
		\end{aligned}
		\label{eq:probit}
		\end{equation}		
		which can also be expressed as:
		\begin{equation}
		P(y \preceq \mathcal{C}_q | \mathbf{x}) = \int_{-\infty}^{b_q - f(\mathbf{x})} \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2}x^2} \mathrm{d}x.
		\label{eq:probit2}
		\end{equation}
		
		\item \textit{Complementary log-log}. Like the \texttt{logit} and the \texttt{probit} transformation, the complementary log-log transformation takes a response that is restricted to the $(0,1)$ interval and converts it into something in the $(-\infty, +\infty)$ interval. Complementary log-log expression is:
		\begin{equation}
		\begin{aligned}
		\log[-\log[1 - P(y \preceq \mathcal{C}_q | \mathbf{x})]] =&\\
		= b_q - f(\mathbf{x}), \quad q = 1, ..., Q-1,&
		\end{aligned}
		\label{eq:cloglog}
		\end{equation}
		that is:
		\begin{equation}
		P(y \preceq \mathcal{C}_q | \mathbf{x}) = 1 - e^{-e^{b_q - f(\mathbf{x})}}, \quad q = 1, ..., Q-1.
		\label{eq:cloglog2}
		\end{equation}
	\end{itemize}
	
	\texttt{Logit} and \texttt{probit} links are symmetric:	
	\begin{equation}
	\text{link}[P(y \preceq \mathcal{C}_q | \mathbf{x})] = -\text{link}[1 - P(y \preceq \mathcal{C}_q | \mathbf{x})],
	\end{equation}
	which means that the response curve for $P(y \preceq \mathcal{C}_q | \mathbf{x})$ is symmetric around the point $P(y \preceq \mathcal{C}_q | \mathbf{x}) = 0.5$, i.e. $P(y \preceq \mathcal{C}_q | \mathbf{x})$ has the same rate when approaching 0 than when approaching 1. This symmetric property can be demonstrated as follows:	
	\begin{enumerate}
		\item Let $P(y \preceq \mathcal{C}_q | \mathbf{x}) \equiv p$. For the \texttt{logit} function, we have:
		\begin{equation}
		\begin{aligned}
		\text{link}(p) = \text{logit}(p) &= \log\left(\frac{p}{1-p}\right) =\\
		&= \log(p) - \log(1-p),
		\end{aligned}
		\end{equation}			
		while:			
		\begin{equation}
		\begin{aligned}
		-\text{link}(1 - p) &= -\text{logit}(1 - p) =\\
		=- \log\left(\frac{1- p}{p}\right) &= - \log(1 - p) + \log(p).
		\end{aligned}
		\end{equation}
		
		\item For the \texttt{probit}:		
		\begin{equation}
		\begin{aligned}
		p \equiv P(y \preceq \mathcal{C}_q | \mathbf{x}) &= \Phi(b_q - f(\mathbf{x})) =\\
		&= \int_{-\infty}^{b_q - f(\mathbf{x})} \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2}x^2} \mathrm{d}x,
		\end{aligned}
		\end{equation}
		which leads to:
		\begin{align}
		\text{probit}(p) & = \Phi^{-1}(p) = b_q - f(\mathbf{x}),\\
		-\text{probit}(1-p) &= \Phi^{-1}(1-p) = -b_q + f(\mathbf{x}),
		\end{align}
		where:		
		\begin{equation}
		1 - p = \int_{-\infty}^{-b_q + f(\mathbf{x})} \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2}x^2} \mathrm{d}x,
		\end{equation}
		\begin{equation}
		p = 1 - \int_{-\infty}^{-b_q + f(\mathbf{x})} \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2}x^2} \mathrm{d}x.
		\end{equation}
	\end{enumerate}
	
	Unlike \texttt{logit} and \texttt{probit}, the complementary log-log model is asymmetrical. It is frequently used when the probability of an event is very small or very large. When the given data is not symmetric in the $[0,1]$ interval and increase slowly at small to moderate value but increases sharply near 1, the \texttt{logit} and \texttt{probit} models are inappropriate. However, in this situation, the complementary log-log model might give a satisfying answer.
	
	\section{Experiments}
	\label{sect:experiments}
	\subsection{Data}
	In order to evaluate the different models, we make use of two ordinal datasets:
	
	\begin{itemize}
		\item \textit{Diabetic Retinopathy (DR)}\footnote{https://www.kaggle.com/c/diabetic-retinopathy-detection/data}. DR is a dataset consisting of extremely high-resolution fundus image data. The training set consists of $17563$ pairs of images (where a  pair consists of a left and right eye image corresponding to a patient). In this dataset, we try to predict the correct category from five levels of diabetic retinopathy: no DR ($25810$ images), mild DR ($2443$ images), moderate DR ($5292$ images), severe DR ($873$ images), or proliferative DR ($708$ images). The test set contains $26788$ pairs of images. These images are taken in variable conditions: by different cameras, varying conditions of illumination and different resolutions. These images come from the EyePACS dataset that was used in a Diabetic Retinopathy Detection competition hosted on the Kaggle platform. Also, this dataset was used in later works~\cite{de2018weighted,nebot2016diabetic}, applying an ordinal QWK cost function in \cite{de2018weighted} to achieve better performance. A validation set is set aside, consisting of $10\%$ of the patients in the training set. The images are resized to 128 by 128 pixels and rescaled to $[0,1]$ range. Data augmentation techniques, described in Section \ref{sect:settings}, are applied to achieve a higher number of samples. A few images of this dataset are shown in Figure \ref{fig:DRexamples}.
		
		\begin{figure}[!t]
			\centering
			\begin{tabular}{cccc}
			\includegraphics[width=1.6cm]{img/retinopathy/1.jpg} & \includegraphics[width=1.6cm]{img/retinopathy/2.jpg} & \includegraphics[width=1.6cm]{img/retinopathy/3.jpg} & \includegraphics[width=1.6cm]{img/retinopathy/4.jpg}\\
			\includegraphics[width=1.6cm]{img/retinopathy/5.jpg} & \includegraphics[width=1.6cm]{img/retinopathy/6.jpg} & \includegraphics[width=1.6cm]{img/retinopathy/7.jpg} & \includegraphics[width=1.6cm]{img/retinopathy/8.jpg}
			\end{tabular}						
			\caption{Examples of the Diabetic Retinopathy test set.}
			\label{fig:DRexamples}
		\end{figure}
		
		
		
		\item \textit{Adience}\footnote{http://www.openu.ac.il/home/hassner/Adience/data.html}. This dataset consists of $26580$ faces belonging to $2284$ subjects. We use the form of the dataset where faces have been pre-cropped and aligned. The dataset was preprocessed, using the methods described in a previous work~\cite{beckham2017unimodal}, so that the images are 256 pixels in width and height, and pixels values follow a $(0;1)$ normal distribution. The original dataset was split into five cross-validation folds. The training set consists of merging the first four folds which comprise a total of $15554$ images. From this, $10\%$ of the images are held out as part of a validation set. The last fold is used as test set. Some images of this dataset are shown in Figure \ref{fig:AdienceExamples}.
		
		\begin{figure}[!t]
			\centering
			\begin{tabular}{cccc}
				\includegraphics[width=1.6cm]{img/adience/1.jpg} & \includegraphics[width=1.6cm]{img/adience/2.jpg} & \includegraphics[width=1.6cm]{img/adience/3.jpg} & \includegraphics[width=1.6cm]{img/adience/4.jpg}\\
				\includegraphics[width=1.6cm]{img/adience/5.jpg} & \includegraphics[width=1.6cm]{img/adience/6.jpg} & \includegraphics[width=1.6cm]{img/adience/7.jpg} & \includegraphics[width=1.6cm]{img/adience/8.jpg}
			\end{tabular}						
			\caption{Examples of the Diabetic Retinopathy test set.}
			\label{fig:AdienceExamples}
		\end{figure}
	\end{itemize}
	
	\subsection{Model}
	\label{sect:model}
	CNNs have been used for both datasets. The different architectures of CNN used in these experiments are presented in tables \ref{table:CNNArchitecture} and \ref{table:ResNetArchitecture}. The architecture for DR is the same that was used in \cite{de2018weighted} and the network for Adience is a small Residual Network (ResNet) \cite{he2016deep} that was used in \cite{beckham2017unimodal}. The most important parameters for convolutional layers are the number of filters that are used to make the convolution operation, the size of these filters and the stride, which is the number of pixels that the filter is moved for obtaining the next pixel. Pooling layers have got similar parameters: the pool size is the number of pixels that will be involved in the pooling operation, and the stride represents the same concept that in convolutional layers. For convolutional layers, ConvWxH@FsS~=~F filters of size WxH and stride S. For pooling layers, PoolWxHsS~=~pool size of WxH and stride S.
	
	The Exponential Linear Unit (ELU)~\cite{clevert2015fast} has been used for activation function for all the convolutional and dense layers, instead of the ReLU~\cite{nair2010rectified} function, as it mitigates the effects of the vanishing gradients problem \cite{bengio1994learning,pascanu2013difficulty} via the identity for positive values. Also, ELUs lead to faster training and better generalization performance than ReLU and Leaky ReLU (LReLU)~\cite{maas2013rectifier} functions on networks with more than five layers.
	
	After every ELU activation function of the convolutional layers, Batch Normalization~\cite{ioffe2015batch} is applied. This method reduces the internal covariate shift by normalizing layer outputs. It allows us to use higher learning rates and be less careful about weights initialization. It also eliminates the need for using regularization techniques like Dropout.
	
	At the output of the network, the CLM is used. Also, a learnable parameter has been used to rescale the projections used by the Cumulative Link Model to make it more stable and guarantee the convergence in most cases. 
	
	\begin{table}[!t]
		\caption{Description of the architecture used in the DR experiments.}
		\label{table:CNNArchitecture}
		\centering
		\begin{tabular}{lc}
			\hline
			\hline
			\textbf{Layer} & \textbf{Output shape}\\
			\hline
			%Conv3x3@32s1 & 254x254x32\\\hline
			2 x Conv3x3@32s1 & 252x252x32\\
			MaxPool2x2s2 & 126x126x32\\
			
			%Conv3x3@64s1 & 124x124x64\\\hline
			2 x Conv3x3@64s1 & 122x122x64\\
			MaxPool2x2s2 & 61x61x64\\
			
			%Conv3x3@128s1 & 59x59x128\\\hline
			2 x Conv3x3@128s1 & 57x57x128\\
			MaxPool2x2s2 & 28x28x128\\
			
			%Conv3x3@128s1 & 26x26x128\\\hline
			2 x Conv3x3@128s1 & 24x24x128\\
			MaxPool2x2s2 & 12x12x128\\
			
			Conv4x4@128s1 & 9x9x128\\
			\hline
			\hline
		\end{tabular}	
	\end{table}
	
	\begin{table}[!t]
		\caption{Description of the architecture used in the Adience experiments.}
		\label{table:ResNetArchitecture}
		\centering
		\begin{tabular}{lc}
			\hline
			\hline
			\textbf{Layer} & \textbf{Output shape}\\
			\hline
			Conv7x7@32s2 & 112x112x32\\
			MaxPool3x3s2 & 55x55x32\\
			2 x ResBlock3x3@64s1 & 55x55x32\\
			1 x ResBlock3x3@128s2 & 28x28x64\\
			2 x ResBlock3x3@128s1 & 28x28x64\\
			1 x ResBlock3x3@256s2 & 14x14x128\\
			2 x ResBlock3x3@256s1 & 14x14x128\\
			1 x ResBlock3x3@512s2 & 7x7x256\\
			2 x ResBlock3x3@512s1 & 7x7x256\\
			AveragePool7x7s2 & 1x1x256\\
			\hline
			\hline
		\end{tabular}	
	\end{table}
	
	\subsection{Experimental design}
	\label{sect:settings}
	The model is optimized using a batch based first-order optimization algorithm called Adam~\cite{kingma2014adam}. We study different initial learning rates ($\eta$) in order to find the optimal one for each problem. We apply an exponential decay across training epochs to the initial learning rate ($\eta_0$) following the expression below:
	\begin{equation}
		\eta = \eta_0 \cdot e^{-0.025 \cdot \text{epoch}}.
	\end{equation}
	
	Fig. \ref{fig:lrdecay} represents the learning rate decay for the different initial values considered in this work.
	\begin{figure}[!t]
		\centering
		\includegraphics[width=3in]{img/lr.pdf}
		\caption{Representation of the learning rate decay.}
		\label{fig:lrdecay}
	\end{figure}
	
	Both datasets are artificially equalized using data augmentation techniques~\cite{van2001art}. However, different transformations are applied to each one. DR dataset augmentation is based on image cropping and zooming, horizontal and vertical flipping, brightness adjustment and random rotations. Horizontal flipping is the only transformation applied to Adience dataset. These transformations are applied every time a new batch is loaded, and the parameters of each one are randomly chosen from a defined range, providing a new set of transformed images for each batch. This technique reduces the overfitting risk and provides an important performance boost as we always work with different but similar images \cite{krizhevsky2012imagenet}.
	
	The epoch size is equal to the number of images in the training set. It could be a higher number as we are using data augmentation, but instead of increasing the epoch size, we rather run the training for more epochs. In this case, we set the maximum number of epochs to $100$. However, we always save the best model, that is evaluated when the training finishes.
	
	The model is evaluated using the Quadratic Weighted Kappa metric (QWK)~\cite{ben2008comparison}. This evaluation measure gives a higher weight to the errors that are further from the correct class. A similar approach to the one described in \cite{de2018weighted} is used as the loss function as it gives better performance for ordinal classification problems. In this case, we define the QWK loss function ($\text{QWK}_l$) as follows:
	\begin{equation}
	\text{QWK}_l = \frac{\sum\limits_{k=1}^N \sum\limits_{q=1}^Q \omega_{t_k, q} P(y = \mathcal{C}_q | \mathbf{x}_k)}{\sum\limits_{i=1}^Q \frac{N_i}{N} \sum\limits_{j=1}^Q ( \omega_{i,j} \sum\limits_{k=1}^N P(y = \mathcal{C}_j | \mathbf{x}_k))},
	\end{equation}
	where $\text{QWK}_l \in [0,2]$, $\mathbf{x}_k$ and $t_k$ are the input data and the real class of the $k$-th sample, $Q$ is the number of classes, $N$ is the number of samples, $N_i$ is the number of samples of the $i$th class, $P(y = \mathcal{C}_q | \mathbf{x}_k)$ is the probability that the $k$th sample belongs to class $\mathcal{C}_q$ and $\omega_{i,j}$ are the elements of the penalization matrix. In this case, $\omega_{i,j} = \frac{(i-j)^2}{(C-1)^2}$, where $\omega_{i,j} \in [0,1]$.
	
	Also, other evaluation metrics are used to ease the comparison with other works: Minimum Sensitivity (MS) \cite{cruz2014metrics}, Mean Absolute Error (MAE) \cite{cruz2014metrics}, Mean Squared Error (MSE) \cite{wang2009mean}, Correct Classification Rate (CCR), Top-2 CCR \cite{beckham2017unimodal}, Top-3 CCR \cite{beckham2017unimodal} and 1-off accuracy \cite{chen2016cascaded,levi2015age,eidinger2014age}.
	
	Experiments are run with the standard cross-entropy loss and the softmax function too in order to prove the performance improvement of considering the ordinality of the problem (QWK loss and the Cumulative Link Model). The results of these experiments are analysed in Section \ref{sect:NominalComparison}.
	
	\subsection{Factors}
	Three different factors are considered: learning rate, batch size and link function for the final output layer.
	
	\begin{itemize}
		\item \textit{Learning rate} (LR, $\eta$). Learning rate is one of the most critical hyper-parameters to tune for training deep neural networks. Optimal learning rate can vary depending on the dataset and the CNN architecture. Previous works have presented some techniques that adjust this parameter in order to achieve better performance~\cite{smith2017cyclical,senior2013empirical}. Within this work, we consider three different values for this parameter: $10^{-2}$, $10^{-3}$ and $10^{-4}$.
		\item \textit{Batch size} (BS). Batch size is also an important parameter as it controls the number of weight updates that are made on every epoch. It can affect the training time and the model performance. In this paper, we try three separate batch sizes for each dataset. For DR dataset, we use $5$, $10$ and $15$ while, for the Adience dataset, $64$, $128$ and $256$ images are used. We took the batch sizes that were used in \cite{de2018weighted} and \cite{beckham2017unimodal} as a reference, and we expanded the range on both sides.
		\item \textit{Link function} (LF). Different link functions are used for the CLM at the last layer output: \texttt{logit}, \texttt{probit} and complementary log-log.
	\end{itemize}
	
	\section{Results}
	\label{sect:results}
	In this section, we present the results of the experiments. For each dataset, we show a table with the detailed results of the experiments performed for training the model with each combination of parameters. Every parameter combination was run five times. These tables show the mean value and the standard deviation (SD) of each metric across these five executions for the test set.
	
	\subsection{Diabetic Retinopathy}
	\label{sect:dr}
	Detailed test results for the Diabetic Retinopathy dataset are presented in Table \ref{table:DRresults}. The best result for each metric is marked in bold and the second best is in italic font.
	
	\begin{table*}[!t]
		\caption{Diabetic Retinopathy results. BS stands for Batch Size, LF for link function and LR for Learning Rate. Mean and standard deviation $\text{Mean}_\text{SD}$.}
		\label{table:DRresults}
		\footnotesize
		\centering
		\begin{tabular}{c@{\hskip 0.15cm}c@{\hskip 0.15cm}c@{\hskip 0.15cm}c@{\hskip 0.30cm}c@{\hskip 0.20cm}c@{\hskip 0.20cm}c@{\hskip 0.20cm}c@{\hskip 0.20cm}c@{\hskip 0.20cm}c@{\hskip 0.20cm}c}
			\hline
			\hline
			BS & LF & LR & $\overline{\text{QWK}}_{{(SD)}}$ & $\overline{\text{MS}}_{{(SD)}}$ & $\overline{\text{MAE}}_{{(SD)}}$ & $\overline{\text{MSE}}_{{(SD)}}$ & $\overline{\text{CCR}}_{{(SD)}}$ & $\overline{\text{Top-2}}_{{(SD)}}$ & $\overline{\text{Top-3}}_{{(SD)}}$ & $\overline{\text{1-off}}_{{(SD)}}$\\\hline
			5 & \texttt{clog-log} & $10^{-2}$ & $0.414_{(0.057)}$ & $0.075_{(0.042)}$ & $0.177_{(0.023)}$ & $0.165_{(0.020)}$ & $0.556_{(0.057)}$ & $0.833_{(0.042)}$ & $0.968_{(0.011)}$ & $0.816_{(0.021)}$\\
			5 & \texttt{clog-log} & $10^{-3}$ & $0.534_{(0.027)}$ & $0.102_{(0.011)}$ & $0.137_{(0.006)}$ & $0.123_{(0.004)}$ & $0.658_{(0.015)}$ & $0.871_{(0.011)}$ & $0.966_{(0.003)}$ & $0.852_{(0.002)}$\\
			5 & \texttt{clog-log} & $10^{-4}$ & $0.520_{(0.006)}$ & $0.067_{(0.008)}$ & $0.123_{(0.003)}$ & $0.104_{(0.001)}$ & $0.697_{(0.006)}$ & $0.842_{(0.008)}$ & $0.961_{(0.003)}$ & $0.851_{(0.002)}$\\
			5 & \texttt{logit} & $10^{-2}$ & $0.416_{(0.041)}$ & $0.095_{(0.029)}$ & $0.175_{(0.021)}$ & $0.162_{(0.018)}$ & $0.563_{(0.054)}$ & $0.762_{(0.040)}$ & $0.908_{(0.026)}$ & $0.807_{(0.029)}$\\
			5 & \texttt{logit} & $10^{-3}$ & $0.554_{(0.013)}$ & $0.093_{(0.009)}$ & $0.137_{(0.003)}$ & $0.123_{(0.003)}$ & $0.660_{(0.008)}$ & $0.802_{(0.005)}$ & $0.936_{(0.004)}$ & $0.853_{(0.005)}$\\
			5 & \texttt{logit} & $10^{-4}$ & $0.520_{(0.003)}$ & $0.063_{(0.004)}$ & $0.122_{(0.002)}$ & $0.102_{(0.001)}$ & $0.706_{(0.005)}$ & $0.823_{(0.004)}$ & $0.949_{(0.003)}$ & $0.862_{(0.003)}$\\
			5 & \texttt{probit} & $10^{-2}$ & $0.460_{(0.048)}$ & $0.079_{(0.046)}$ & $0.197_{(0.064)}$ & $0.182_{(0.053)}$ & $0.504_{(0.167)}$ & $0.808_{(0.034)}$ & $0.927_{(0.073)}$ & $0.689_{(0.240)}$\\
			5 & \texttt{probit} & $10^{-3}$ & $0.564_{(0.018)}$ & $0.099_{(0.013)}$ & $0.147_{(0.018)}$ & $0.132_{(0.014)}$ & $0.636_{(0.045)}$ & $0.822_{(0.040)}$ & $0.939_{(0.020)}$ & $0.840_{(0.015)}$\\
			5 & \texttt{probit} & $10^{-4}$ & $0.523_{(0.005)}$ & $0.067_{(0.012)}$ & $0.122_{(0.002)}$ & $0.105_{(0.001)}$ & $0.701_{(0.006)}$ & $0.823_{(0.002)}$ & $0.953_{(0.002)}$ & $0.860_{(0.003)}$\\
			10 & \texttt{clog-log} & $10^{-2}$ & $0.423_{(0.239)}$ & $0.062_{(0.051)}$ & $0.127_{(0.017)}$ & $0.120_{(0.014)}$ & $0.684_{(0.046)}$ & $\mathbf{0.894_{(0.062)}}$ & $\mathbf{0.986_{(0.012)}}$ & $0.832_{(0.020)}$\\
			10 & \texttt{clog-log} & $10^{-3}$ & $\mathbf{0.582_{(0.016)}}$ & $0.102_{(0.006)}$ & $0.128_{(0.003)}$ & $0.115_{(0.002)}$ & $0.680_{(0.007)}$ & $\mathit{0.880_{(0.004)}}$ & $0.972_{(0.003)}$ & $0.861_{(0.004)}$\\
			10 & \texttt{clog-log} & $10^{-4}$ & $0.537_{(0.010)}$ & $0.064_{(0.004)}$ & $\mathit{0.116_{(0.001)}}$ & $0.096_{(0.001)}$ & $0.717_{(0.003)}$ & $0.837_{(0.002)}$ & $0.971_{(0.001)}$ & $0.860_{(0.002)}$\\
			10 & \texttt{logit} & $10^{-2}$ & $0.531_{(0.031)}$ & $0.107_{(0.008)}$ & $0.151_{(0.010)}$ & $0.140_{(0.008)}$ & $0.623_{(0.025)}$ & $0.802_{(0.022)}$ & $0.934_{(0.013)}$ & $0.838_{(0.014)}$\\
			10 & \texttt{logit} & $10^{-3}$ & $0.579_{(0.009)}$ & $0.096_{(0.012)}$ & $0.127_{(0.005)}$ & $0.113_{(0.004)}$ & $0.686_{(0.013)}$ & $0.817_{(0.006)}$ & $0.954_{(0.005)}$ & $0.861_{(0.002)}$\\
			10 & \texttt{logit} & $10^{-4}$ & $0.539_{(0.007)}$ & $0.074_{(0.013)}$ & $0.126_{(0.005)}$ & $0.095_{(0.002)}$ & $0.707_{(0.010)}$ & $0.823_{(0.007)}$ & $0.957_{(0.005)}$ & $0.858_{(0.004)}$\\
			10 & \texttt{probit} & $10^{-2}$ & $0.508_{(0.037)}$ & $0.088_{(0.044)}$ & $0.145_{(0.018)}$ & $0.137_{(0.014)}$ & $0.639_{(0.045)}$ & $0.835_{(0.015)}$ & $0.960_{(0.008)}$ & $0.829_{(0.020)}$\\
			10 & \texttt{probit} & $10^{-3}$ & $0.558_{(0.034)}$ & $\mathbf{0.111_{(0.005)}}$ & $0.134_{(0.003)}$ & $0.120_{(0.002)}$ & $0.666_{(0.008)}$ & $0.831_{(0.007)}$ & $0.955_{(0.001)}$ & $0.863_{(0.003)}$\\
			10 & \texttt{probit} & $10^{-4}$ & $0.541_{(0.010)}$ & $0.076_{(0.006)}$ & $0.119_{(0.002)}$ & $0.098_{(0.001)}$ & $0.712_{(0.005)}$ & $0.828_{(0.003)}$ & $0.961_{(0.002)}$ & $0.862_{(0.001)}$\\
			15 & \texttt{clog-log} & $10^{-2}$ & $0.564_{(0.016)}$ & $0.108_{(0.014)}$ & $0.143_{(0.006)}$ & $0.134_{(0.005)}$ & $0.640_{(0.015)}$ & $0.879_{(0.011)}$ & $0.972_{(0.005)}$ & $0.851_{(0.006)}$\\
			15 & \texttt{clog-log}g & $10^{-3}$ & $0.559_{(0.026)}$ & $\mathit{0.111_{(0.008)}}$ & $0.127_{(0.004)}$ & $0.113_{(0.003)}$ & $0.682_{(0.010)}$ & $0.871_{(0.008)}$ & $\mathit{0.974_{(0.002)}}$ & $\mathbf{0.868_{(0.002)}}$\\
			15 & \texttt{clog-log} & $10^{-4}$ & $0.538_{(0.009)}$ & $0.054_{(0.003)}$ & $\mathbf{0.115_{(0.002)}}$ & $\mathit{0.093_{(0.001)}}$ & $0.720_{(0.006)}$ & $0.835_{(0.007)}$ & $0.970_{(0.003)}$ & $0.860_{(0.006)}$\\
			15 & \texttt{logit} & $10^{-2}$ & $0.551_{(0.020)}$ & $0.104_{(0.008)}$ & $0.139_{(0.011)}$ & $0.129_{(0.009)}$ & $0.654_{(0.027)}$ & $0.815_{(0.017)}$ & $0.948_{(0.016)}$ & $0.856_{(0.015)}$\\
			15 & \texttt{logit} & $10^{-3}$ & $0.551_{(0.010)}$ & $0.106_{(0.016)}$ & $0.129_{(0.008)}$ & $0.114_{(0.005)}$ & $0.680_{(0.019)}$ & $0.818_{(0.008)}$ & $0.952_{(0.007)}$ & $\mathit{0.866_{(0.001)}}$\\
			15 & \texttt{logit} & $10^{-4}$ & $0.543_{(0.008)}$ & $0.056_{(0.003)}$ & $0.121_{(0.004)}$ & $\mathbf{0.090_{(0.001)}}$ & $\mathbf{0.723_{(0.004)}}$ & $0.833_{(0.004)}$ & $0.964_{(0.003)}$ & $0.862_{(0.004)}$\\
			15 & \texttt{probit} & $10^{-2}$ & $0.534_{(0.032)}$ & $0.104_{(0.013)}$ & $0.148_{(0.015)}$ & $0.138_{(0.014)}$ & $0.631_{(0.038)}$ & $0.845_{(0.030)}$ & $0.964_{(0.010)}$ & $0.852_{(0.010)}$\\
			15 & \texttt{probit} & $10^{-3}$ & $\mathit{0.580_{(0.021)}}$ & $0.104_{(0.016)}$ & $0.129_{(0.008)}$ & $0.116_{(0.005)}$ & $0.680_{(0.018)}$ & $0.832_{(0.010)}$ & $0.959_{(0.007)}$ & $0.866_{(0.003)}$\\
			15 & \texttt{probit} & $10^{-4}$ & $0.533_{(0.004)}$ & $0.065_{(0.005)}$ & $0.117_{(0.002)}$ & $0.094_{(0.001)}$ & $\mathit{0.721_{(0.004)}}$ & $0.832_{(0.002)}$ & $0.964_{(0.001)}$ & $0.863_{(0.001)}$\\
			\hline
			\hline
		\end{tabular}
	\end{table*}
	
	The best mean QWK value was obtained with the complementary log-log link function using a batch size of 10 and a learning rate of $10^{-3}$. However, the best CCR value was obtained with a batch size of 15, the \texttt{logit} link and a learning rate of $10^{-4}$. The optimal configuration depends on the metric we are analysing. In this case, as we are working with an ordinal problem, the most reliable metric is the QWK. However, the rest of the metrics are also included to allow further comparisons with future works.
	
	\subsection{Adience}
	\label{sect:adience}
	Test results for the experiments made with the Adience dataset are shown in Table \ref{table:AdienceTest}. The best result for each metric is marked in bold and the second best is in italic font.
	
	\begin{table*}[!t]
		\caption{Adience test results. BS stands for Batch Size, LF for link function and LR for Learning Rate. Mean and standard deviation $\text{Mean}_\text{SD}$.}
		\label{table:AdienceTest}
		\footnotesize
		\centering
		\begin{tabular}{c@{\hskip 0.15cm}c@{\hskip 0.15cm}c@{\hskip 0.15cm}c@{\hskip 0.30cm}c@{\hskip 0.20cm}c@{\hskip 0.20cm}c@{\hskip 0.20cm}c@{\hskip 0.20cm}c@{\hskip 0.20cm}c@{\hskip 0.20cm}c}
			\hline
			\hline
			BS & LF & LR & $\overline{\text{QWK}}_{{(SD)}}$ & $\overline{\text{MS}}_{{(SD)}}$ & $\overline{\text{MAE}}_{{(SD)}}$ & $\overline{\text{MSE}}_{{(SD)}}$ & $\overline{\text{CCR}}_{{(SD)}}$ & $\overline{\text{Top-2}}_{{(SD)}}$ & $\overline{\text{Top-3}}_{{(SD)}}$ & $\overline{\text{1-off}}_{{(SD)}}$\\\hline
			64 & \texttt{clog-log} & $10^{-2}$ & $0.808_{(0.025)}$ & $0.086_{(0.041)}$ & $0.147_{(0.008)}$ & $0.129_{(0.007)}$ & $0.415_{(0.031)}$ & $0.677_{(0.024)}$ & $0.798_{(0.036)}$ & $0.804_{(0.015)}$\\
			64 & \texttt{clog-log} & $10^{-3}$ & $0.873_{(0.006)}$ & $0.144_{(0.057)}$ & $\mathbf{0.124_{(0.003)}}$ & $0.101_{(0.003)}$ & $\mathbf{0.519_{(0.014)}}$ & $\mathit{0.764_{(0.010)}}$ & $0.861_{(0.019)}$ & $0.886_{(0.006)}$\\
			64 & \texttt{clog-log} & $10^{-4}$ & $0.799_{(0.010)}$ & $0.000_{(0.000)}$ & $0.174_{(0.001)}$ & $0.100_{(0.002)}$ & $0.324_{(0.015)}$ & $0.616_{(0.020)}$ & $0.795_{(0.012)}$ & $0.771_{(0.014)}$\\
			64 & \texttt{logit} & $10^{-2}$ & $0.778_{(0.019)}$ & $0.074_{(0.041)}$ & $0.159_{(0.006)}$ & $0.137_{(0.007)}$ & $0.366_{(0.025)}$ & $0.636_{(0.015)}$ & $0.785_{(0.010)}$ & $0.775_{(0.015)}$\\
			64 & \texttt{logit} & $10^{-3}$ & $\mathbf{0.881_{(0.005)}}$ & $\mathit{0.178_{(0.023)}}$ & $\mathit{0.126_{(0.001)}}$ & $0.098_{(0.003)}$ & $\mathit{0.518_{(0.008)}}$ & $\mathbf{0.765_{(0.015)}}$ & $\mathbf{0.902_{(0.005)}}$ & $\mathbf{0.894_{(0.005)}}$\\
			64 & \texttt{logit} & $10^{-4}$ & $0.784_{(0.011)}$ & $0.000_{(0.000)}$ & $0.180_{(0.001)}$ & $0.108_{(0.004)}$ & $0.318_{(0.026)}$ & $0.621_{(0.034)}$ & $0.772_{(0.024)}$ & $0.731_{(0.030)}$\\
			64 & \texttt{probit} & $10^{-2}$ & $0.836_{(0.005)}$ & $0.135_{(0.021)}$ & $0.134_{(0.002)}$ & $0.121_{(0.002)}$ & $0.468_{(0.011)}$ & $0.720_{(0.009)}$ & $0.861_{(0.009)}$ & $0.829_{(0.005)}$\\
			64 & \texttt{probit} & $10^{-3}$ & $\mathit{0.874_{(0.004)}}$ & $0.134_{(0.012)}$ & $0.126_{(0.003)}$ & $0.105_{(0.003)}$ & $0.511_{(0.014)}$ & $0.756_{(0.009)}$ & $\mathit{0.895_{(0.003)}}$ & $\mathit{0.889_{(0.003)}}$\\
			64 & \texttt{probit} & $10^{-4}$ & $0.805_{(0.004)}$ & $0.000_{(0.000)}$ & $0.170_{(0.001)}$ & $0.100_{(0.002)}$ & $0.360_{(0.011)}$ & $0.653_{(0.011)}$ & $0.809_{(0.009)}$ & $0.790_{(0.009)}$\\
			128 & \texttt{clog-log} & $10^{-2}$ & $0.832_{(0.013)}$ & $0.123_{(0.031)}$ & $0.135_{(0.004)}$ & $0.117_{(0.002)}$ & $0.463_{(0.013)}$ & $0.705_{(0.019)}$ & $0.813_{(0.025)}$ & $0.832_{(0.006)}$\\
			128 & \texttt{clog-log} & $10^{-3}$ & $0.873_{(0.006)}$ & $\mathbf{0.185_{(0.029)}}$ & $0.128_{(0.002)}$ & $0.100_{(0.001)}$ & $0.513_{(0.007)}$ & $0.758_{(0.008)}$ & $0.870_{(0.011)}$ & $0.880_{(0.009)}$\\
			128 & \texttt{clog-log} & $10^{-4}$ & $0.659_{(0.025)}$ & $0.000_{(0.000)}$ & $0.190_{(0.002)}$ & $0.125_{(0.004)}$ & $0.235_{(0.026)}$ & $0.466_{(0.031)}$ & $0.640_{(0.030)}$ & $0.536_{(0.041)}$\\
			128 & \texttt{logit} & $10^{-2}$ & $0.781_{(0.041)}$ & $0.096_{(0.059)}$ & $0.153_{(0.007)}$ & $0.125_{(0.007)}$ & $0.398_{(0.031)}$ & $0.638_{(0.033)}$ & $0.790_{(0.025)}$ & $0.779_{(0.020)}$\\
			128 & \texttt{logit} & $10^{-3}$ & $0.865_{(0.005)}$ & $0.127_{(0.026)}$ & $0.134_{(0.001)}$ & $0.099_{(0.003)}$ & $0.497_{(0.009)}$ & $0.754_{(0.008)}$ & $0.882_{(0.009)}$ & $0.874_{(0.008)}$\\
			128 & \texttt{logit} & $10^{-4}$ & $0.586_{(0.008)}$ & $0.000_{(0.000)}$ & $0.196_{(0.001)}$ & $0.151_{(0.005)}$ & $0.192_{(0.001)}$ & $0.364_{(0.060)}$ & $0.581_{(0.034)}$ & $0.396_{(0.002)}$\\
			128 & \texttt{probit} & $10^{-2}$ & $0.849_{(0.005)}$ & $0.132_{(0.010)}$ & $0.131_{(0.001)}$ & $0.115_{(0.001)}$ & $0.479_{(0.004)}$ & $0.728_{(0.007)}$ & $0.854_{(0.009)}$ & $0.847_{(0.007)}$\\
			128 & \texttt{probit} & $10^{-3}$ & $0.866_{(0.002)}$ & $0.124_{(0.043)}$ & $0.130_{(0.002)}$ & $0.100_{(0.003)}$ & $0.505_{(0.006)}$ & $0.750_{(0.010)}$ & $0.882_{(0.004)}$ & $0.873_{(0.006)}$\\
			128 & \texttt{probit} & $10^{-4}$ & $0.718_{(0.015)}$ & $0.000_{(0.000)}$ & $0.185_{(0.001)}$ & $0.110_{(0.002)}$ & $0.300_{(0.031)}$ & $0.575_{(0.015)}$ & $0.733_{(0.010)}$ & $0.640_{(0.033)}$\\
			256 & \texttt{clog-log} & $10^{-2}$ & $0.853_{(0.004)}$ & $0.157_{(0.024)}$ & $0.130_{(0.002)}$ & $0.110_{(0.001)}$ & $0.485_{(0.009)}$ & $0.744_{(0.006)}$ & $0.842_{(0.016)}$ & $0.858_{(0.004)}$\\
			256 & \texttt{clog-log} & $10^{-3}$ & $0.840_{(0.017)}$ & $0.095_{(0.017)}$ & $0.144_{(0.005)}$ & $0.097_{(0.004)}$ & $0.456_{(0.021)}$ & $0.720_{(0.022)}$ & $0.840_{(0.018)}$ & $0.842_{(0.018)}$\\
			256 & \texttt{clog-log} & $10^{-4}$ & $0.552_{(0.010)}$ & $0.000_{(0.000)}$ & $0.199_{(0.001)}$ & $0.165_{(0.004)}$ & $0.187_{(0.001)}$ & $0.368_{(0.022)}$ & $0.475_{(0.025)}$ & $0.387_{(0.001)}$\\
			256 & \texttt{logit} & $10^{-2}$ & $0.764_{(0.102)}$ & $0.077_{(0.067)}$ & $0.155_{(0.020)}$ & $0.125_{(0.015)}$ & $0.387_{(0.083)}$ & $0.632_{(0.103)}$ & $0.790_{(0.077)}$ & $0.783_{(0.065)}$\\
			256 & \texttt{logit} & $10^{-3}$ & $0.851_{(0.008)}$ & $0.100_{(0.030)}$ & $0.147_{(0.003)}$ & $\mathbf{0.094_{(0.002)}}$ & $0.449_{(0.015)}$ & $0.726_{(0.015)}$ & $0.861_{(0.006)}$ & $0.850_{(0.008)}$\\
			256 & \texttt{logit} & $10^{-4}$ & $0.558_{(0.008)}$ & $0.000_{(0.000)}$ & $0.202_{(0.001)}$ & $0.191_{(0.002)}$ & $0.187_{(0.002)}$ & $0.206_{(0.007)}$ & $0.395_{(0.046)}$ & $0.389_{(0.003)}$\\
			256 & \texttt{probit} & $10^{-2}$ & $0.858_{(0.005)}$ & $0.164_{(0.033)}$ & $0.130_{(0.002)}$ & $0.112_{(0.002)}$ & $0.486_{(0.007)}$ & $0.741_{(0.008)}$ & $0.867_{(0.008)}$ & $0.862_{(0.005)}$\\
			256 & \texttt{probit} & $10^{-3}$ & $0.850_{(0.008)}$ & $0.111_{(0.040)}$ & $0.144_{(0.002)}$ & $\mathit{0.095_{(0.001)}}$ & $0.460_{(0.011)}$ & $0.732_{(0.006)}$ & $0.865_{(0.006)}$ & $0.853_{(0.007)}$\\
			256 & \texttt{probit} & $10^{-4}$ & $0.565_{(0.010)}$ & $0.000_{(0.000)}$ & $0.196_{(0.001)}$ & $0.150_{(0.005)}$ & $0.189_{(0.001)}$ & $0.409_{(0.014)}$ & $0.602_{(0.022)}$ & $0.392_{(0.002)}$\\
			\hline
			\hline
		\end{tabular}
	\end{table*}
	
	The best mean QWK value was obtained with the \texttt{logit} link function using a batch size of 64 and a learning rate of $10^{-4}$. Also, this configuration obtained the best score for Top-2, Top-3 and 1-off accuracy, and the second best for MS, MAE and CCR. In this case, this configuration can be selected as the optimal for this problem.
	
	\subsection{Statistical analysis}
	\label{sect:statisticalanalysis}
	In this subsection, a statistical analysis will be performed in order to obtain conclusions from the result.
	
	The significance and relative importance of the parameters concerning the results obtained, as well as suitable values for each of them, were obtained using an ANalysis Of the VAriance (ANOVA).
	
	The ANOVA test~\cite{miller1997beyond} is one of the most widely used statistical techniques. ANOVA is essentially a method of analysing the variance to which a response is subject into its various components, corresponding to the sources of variation which can be identified.
	
	ANOVA, in this case, examines the effects of three quantitative variables (termed factors) on one quantitative response. Considered factors are the link function, the learning rate for the Adam optimization algorithm, and the batch size. 
	
	Following the setup of the previous study, we performed an ANOVA III analysis and multiple comparison tests. We assume that five executions are enough to do the statistical tests because of the computational time limitations.
	
	We denote by $\text{QWK}_{i,j,k,l}(i=1, ..., 3; j = 1, ..., 3; k = 1, ..., 3)$ the value observed when the first factor is at the $i$-th level, the second at the $j$-th level and the third at the $k$-th level. We assume that the three factors do not act independently and therefore there exists an interaction between each pair of them and between the three factors. In this case, the observations fit:
	\begin{equation}
	\begin{aligned}
	\footnotesize
	\text{QWK}_{i,j,k,l} = \mu & + L_i + P_j + B_k + LP_{i,j} + LB_{i,k} \\& + PB_{j,k} + LPB_{i,j,k} + \epsilon_{i,j,k,l},
	\label{eq:anova3}
	\end{aligned}
	\end{equation}
	where $\mu$ is the fixed effect that is common to all the populations; $L_i$ is the effect associated with the \textit{i-th} level of the link factor (\texttt{logit}, \texttt{probit}, complementary log-log); $P_j$ is the effect associated with the \textit{j-th} level of the learning rate factor and $B_k$ is the effect associated with the \textit{k-th} level of the batch size factor. The term $LP_{i,j}$ denotes the joint effect of the presence of level $i$ of the first factor and level $j$ of the second one; this, therefore, is denominated the interaction term between $L$ and $P$ factors. The same interaction effect is appreciated on $LB_{i,k}$, $PB_{j,k}$ and $LPB_{i,j,k}$. The term $\epsilon_{i,j,k,l}$ is the influence on the result of everything that could not be assigned or of random factors. $QWK_{i,j,k,l}$ is the quadratic weighted kappa measure, the response variable used to perform the statistical analysis.
	
	We consider some hypotheses testing where the null hypothesis is proposed that each term of the above equation is independent of the levels involved. The hypotheses for the levels of the L factor are $H_0 \equiv L_1 = L_2 = L_3,$ and $H_1 \equiv \text{ some } L_i \text{ is different}$.
	
	The same hypotheses are made for the other factors. In this way, we test in the null hypothesis that all of the population means are equal against an alternative hypothesis that there is at least one mean that is not equal to the others.
	
	The hypothesis associated with the interaction between $L$ and $P$ is $H_0 \equiv LP_{i,j} = 0, \forall i,j$, and $H_1 \equiv \exists LP_{i,j} \ne 0$. Similar hypotheses can be assumed for the interaction between the other factors.
	
	The analysis of variance table represents the initial study in a compact form, containing the sum of squares, degrees of freedom, mean square, test statistics and significance level, where non-significative factors and interactions have been removed ($\text{p-value} > 0.05$). These factors and interactions take part of the error component now. In this way, the results of the ANOVA III test for the Diabetic Retinopathy dataset are summarised in Table  \ref{table:ANOVARetinopathy}. There are significant differences in average QWK depending on the link function and also depending on the learning rate for $\alpha=0.05$ ($\text{p-value} = 0.000$). Moreover, an interaction between the link function and the learning rate can be recognised ($\text{p-value} = 0.001$). It means that the learning rate and the link functions have a significant impact on the optimization algorithm results.
		
	\begin{table}[!t]
		\caption{ANOVA III for the analysis of the main factors in the design of a Convolutional Ordinal Neural Network for the Retinopathy dataset.}
		\label{table:ANOVARetinopathy}
		\centering
		\small
		\begin{tabular}{lccccc}
			\hline
			\hline
			           \multicolumn{6}{c}{Response variable QWK}             \\ \hline
			Source       &   S.S.   & D.F.  &  M.S.   &  F-ratio   &  Sig.   \\ \hline
			Model        & $37.860$ &  $9$  & $4.207$ & $1562.840$ & $0.000$ \\
			$L$ factor   & $0.057$  &  $2$  & $0.029$ &  $10.646$  & $0.000$ \\
			$P$ factor   & $0.121$  &  $2$  & $0.060$ &  $22.468$  & $0.000$ \\
			$LP$ factors & $0.057$  &  $4$  & $0.014$ &  $5.261$   & $0.001$ \\
			Error        & $0.339$  & $126$ & $0.003$ &            &  \\ \hline
			Total        & $38.199$ & $135$ &         &            & \\
			\hline
			\hline
		\end{tabular}
	\end{table}
	
	Given that there exist significant differences between the means, we analyse now those differences. A post-hoc multiple comparison test has been performed on the mean QWK obtained. An HSD Tukey's test \cite{tukey1949comparing} was made under the null hypothesis that the variance of the error of the dependent variable is the same between the groups. The results of this test over the test set are shown in Table \ref{table:TukeyDR}. They show that the best link function is the complementary log-log but the \texttt{probit} link performance is close to it. Also, the best value for the learning rate parameter is $10^{-3}$. The batch size is not relevant for this dataset with the values considered.
		
	\begin{table}[!t]
		\caption{Tukey's test results for the Diabetic Retinopathy dataset.}
		\label{table:TukeyDR}
		\centering
		\begin{tabular}{cccc}
			\hline\hline
			       LF         &        LF         & Mean diff. &  Sig.   \\ \hline
			 \texttt{logit}   &  \texttt{probit}  &  $-0.002$  & $0.011$ \\
			                  & \texttt{clog-log} &  $0.012$   & $0.000$ \\
			 \texttt{probit}  &  \texttt{logit}   &  $0.002$   & $0.011$ \\
			                  & \texttt{clog-log} &  $0.014$   & $0.248$ \\
			\texttt{clog-log} &  \texttt{logit}   &  $-0.012$  & $0.000$ \\
			                  &  \texttt{probit}  &  $-0.014$  & $0.248$ \\ \hline\hline
			       LR         &        LR         & Mean diff. &  Sig.   \\ \hline
			    $10^{-2}$     &     $10^{-3}$     &  $-0.073$  & $0.000$ \\
			                  &     $10^{-4}$     &  $-0.044$  & $0.000$ \\
			    $10^{-3}$     &     $10^{-2}$     &  $0.073$   & $0.000$ \\
			                  &     $10^{-4}$     &  $0.029$   & $0.023$ \\
			    $10^{-4}$     &     $10^{-2}$     &  $0.044$   & $0.000$ \\
			                  &     $10^{-3}$     &  $-0.029$  & $0.023$ \\ \hline\hline
		\end{tabular}
	\end{table}
	
	\begin{table}[!t]
		\caption{ANOVA III for the analysis of the main factors in the design of a Convolutional Ordinal Neural Network for the Adience dataset.}
		\label{table:ANOVAAdience}
		\centering
		\small
		\begin{tabular}{lccccc}
			\hline\hline
			            \multicolumn{6}{c}{Response variable QWK}             \\ \hline
			Source        &   S.S.   & D.F.  &  M.S.   &  F-ratio   &  Sig.   \\ \hline
			Model         & $84.372$ & $27$  & $3.125$ & $4414.006$ & $0.000$ \\
			$L$ factor    & $0.156$  &  $2$  & $0.078$ & $110.103$  & $0.000$ \\
			$P$ factor    & $0.925$  &  $2$  & $0.462$ & $653.163$  & $0.000$ \\
			$B$ factor    & $0.040$  &  $2$  & $0.020$ &  $28.218$  & $0.000$ \\
			$LP$ factors  & $0.284$  &  $4$  & $0.071$ & $100.118$  & $0.000$ \\
			$LB$ factors  & $0.008$  &  $4$  & $0.002$ &  $2.837$   & $0.028$ \\
			$PB$ factors  & $0.026$  &  $4$  & $0.007$ &  $9.267$   & $0.000$ \\
			$LPB$ factors & $0.021$  &  $8$  & $0.003$ &  $3.728$   & $0.001$ \\
			Error         & $0.076$  & $108$ & $0.001$ &            &  \\ \hline
			Total         & $84.449$ & $135$ &         &            &  \\ \hline\hline
		\end{tabular}
	\end{table}
	
	The results of the ANOVA III test for the Adience dataset are shown in Table \ref{table:ANOVAAdience}. First, we observe that there exist significant differences in average QWK concerning the three factors ($\text{p-value} = 0.000$). Secondly, we found interactions between all the pairs of factors and between all the three factors together (p-values $0.000$, $0.000$, $0.000$ and $0.001$). It means that the joint action of two or three factors significantly affects the results obtained by the algorithm.
	
	As we did for the DR dataset, a post-hoc multiple comparison test has been performed on the average QWK obtained for Adience. Under the null hypothesis that the variance of the error of the dependent variable is the same between groups, an HSD Tukey's test has been applied. The results of this test over the test set are shown in Table \ref{table:TukeyAdience}.
	
	\begin{table}[!t]
		\caption{Tukey's test results for the Adience dataset.}
		\label{table:TukeyAdience}
		\centering
		\begin{tabular}{cccc}
			\hline\hline
			       LF         &        LF         & Mean diff. &  Sig.   \\ \hline
			 \texttt{logit}   &  \texttt{probit}  &  $0.046$   & $0.000$ \\
			                  & \texttt{clog-log} &  $0.084$   & $0.000$ \\
			 \texttt{probit}  &  \texttt{logit}   &  $-0.046$  & $0.000$ \\
			                  & \texttt{clog-log} &  $0.038$   & $0.000$ \\
			\texttt{clog-log} &  \texttt{logit}   &  $-0.084$  & $0.000$ \\
			                  &  \texttt{probit}  &  $-0.038$  & $0.000$ \\ \hline\hline
			       LR         &        LR         & Mean diff. &  Sig.   \\ \hline
			    $10^{-2}$     &     $10^{-3}$     &  $-0.046$  & $0.000$ \\
			                  &     $10^{-4}$     &  $0.148$   & $0.000$ \\
			    $10^{-3}$     &     $10^{-2}$     &  $0.046$   & $0.000$ \\
			                  &     $10^{-4}$     &  $0.194$   & $0.000$ \\
			    $10^{-4}$     &     $10^{-2}$     &  $-0.148$  & $0.000$ \\
			                  &     $10^{-3}$     &  $-0.194$  & $0.000$ \\ \hline\hline
			       BS         &        BS         & Mean diff. &  Sig.   \\ \hline
			       64         &        128        &  $-0.041$  & $0.026$ \\
			                  &        256        &  $-0.027$  & $0.000$ \\
			       128        &        64         &  $0.041$   & $0.026$ \\
			                  &        256        &  $0.014$   & $0.000$ \\
			       256        &        64         &  $0.027$   & $0.000$ \\
			                  &        128        &  $-0.014$  & $0.000$ \\ \hline\hline
		\end{tabular}
	\end{table}
		
	The results over the test set show that the best link function is the \texttt{logit} one, the best learning rate is $10^{-3}$ and the best batch size is 128. However, the interactions between these factors made the configuration that uses a \texttt{logit} link, $\eta=10^{-3}$ and batch size of 64, the best configuration. It obtained a mean QWK value of $0.940$ for validation and $0.881$ for test. The same parameters, but using the \texttt{probit} link, achieves the second best result ($0.874$). The standard deviation is very low for both cases.
	
	To sum up, the results showed that the best parameter configuration depends on the problem that is being solved. The optimal value for the batch size and the optimal link function are not the same for Retinopathy and Adience datasets. These results highlight the importance of adjusting the hyper-parameters for each problem instead of trying to find an optimal configuration for all the datasets. However, the best learning rate for both datasets were $10^{-3}$. It is recommended to use this value for future datasets.
	The best batch size for DR was 10 while the best value for Adience was 128 (intermediate values considered).
	Finally, there are more interactions between the three factors for the Adience dataset than for the Diabetic Retinopathy one. This highlights the importance of making experimental designs associated with each dataset to determine the best value for each factor.
	
	\subsection{Comparison with nominal method and previous works}
	\label{sect:NominalComparison}
	
	The same experiments described in Section \ref{sect:experiments} were repeated using the cross-entropy instead of the QWK as loss function for the optimizer and the softmax function instead of the Cumulative Link Model for the output of the network. The evaluation metrics remains the same in order to be able to compare. There are some parameter configurations where the training process gets stagnated and a very low QWK is obtained. As we saw in Sections \ref{sect:dr} and \ref{sect:adience}, this problem is not found when using the ordinal method.
	
	For the Diabetic Retinopathy dataset, the best mean value of QWK was $0.497$ and was obtained when using a batch size of 10 and a learning rate of $10^{-4}$.
	
	In the case of Adience dataset, the highest QWK was $0.787$ and was achieved with a batch size of 64 and a learning rate of $10^{-3}$.

	Finally, a comparison of the best results for each dataset for ordinal and nominal cases and previous works is shown in tables \ref{table:ComparisonDR} and \ref{table:ComparisonAdience}. All the results are given for the test set, except those from \cite{de2018weighted} (DR dataset), because the authors only provided validation results for $128\times 128$ images (however, validation results are usually better than test results). The proposed ordinal model outperforms all the other alternatives in terms of QWK. The performance gain of CLM over the softmax reaches $16.8\%$ for Diabetic Retinopathy and $11.9\%$ for Adience dataset. The improvement of the ordinal method for Retinopathy dataset is higher than for Adience dataset. It seems that the method proposed in this work offers a more significant improvement as the given problem complexity increases.
	
	\begin{table}[!t]
		\caption{Comparison between the best results of nominal, ordinal and previous works for the Diabetic Retinopathy dataset.}
		\label{table:ComparisonDR}
		\scriptsize
		\centering
		\def\arraystretch{1.3}
		\begin{tabular}{lccc}
			\hline
			\hline
			Method                                   & $\overline{QWK}_{(SD)}$ & $\overline{CCR}_{(SD)}$ & $\overline{\text{1-off}}_{(SD)}$ \\ \hline
			Ordinal network                          &    $0.582_{(0.016)}$    &    $0.723_{(0.004)}$    &        $0.868_{(0.002)}$         \\
			Nominal network                          &    $0.498_{(0.011)}$    &    $0.692_{(0.012)}$    &        $0.854_{(0.006)}$         \\
			J. Torre et al. \cite{de2018weighted}    &  $0.537_{(\text{-})}$   &            -            &                -                 \\
			À. Nebot et al. \cite{nebot2016diabetic} &  $0.555_{(\text{-})}$   &            -            &                -					\\
			\hline
			\hline
		\end{tabular}
	\end{table}
	
	\begin{table}[!t]
		\caption{Comparison between the best results of nominal, ordinal and previous works for the Adience dataset.}
		\label{table:ComparisonAdience}
		\scriptsize
		\centering
		\def\arraystretch{1.3}
		\begin{tabular}{lccc}
			\hline
			\hline
			Method                                    & $\overline{QWK}_{(SD)}$ & $\overline{CCR}_{(SD)}$ & $\overline{\text{1-off}}_{(SD)}$ \\ \hline
			Ordinal network                           &    $0.881_{(0.005)}$    &    $0.519_{(0.013)}$    &        $0.894_{(0.005)}$         \\
			Nominal network                           &    $0.787_{(0.004)}$    &    $0.458_{(0.008)}$    &        $0.800_{(0.007)}$         \\
			E. Eidinger et al. \cite{eidinger2014age} &            -            &    $0.451_{(0.026)}$    &        $0.807_{(0.011)}$         \\
			J.-C. Chen et al. \cite{chen2016cascaded} &            -            &    $0.529_{(0.060)}$    &        $0.885_{(0.022)}$         \\
			G. Levi et al. \cite{levi2015age}         &            -            &    $0.507_{(0.051)}$    &        $0.847_{(0.022)}$		 \\
			\hline
			\hline
		\end{tabular}
	\end{table}
	
	\section{Conclusions}
	\label{sect:conclusions}
	The first conclusion obtained from the results of our experiments is that the optimal values for the different parameters considered are problem-dependant. 
	
	The complementary log-log function offers the best results in Diabetic Retinopathy dataset while the \texttt{logit} link is the best option for the Adience dataset. These results provide an opportunity for exploring new generalised link functions that could be dynamically adapted to any problem.
	
	The best value for the learning rate parameter for both datasets is $\eta = 10^{-3}$. It can be considered a good value for this parameter when training the model with new datasets.
	
	Both datasets have obtained the best performance with an intermediate batch size: 10 for Diabetic Retinopathy and 128 for Adience.
	
	Also, the statistical tests reported that there are relevant interactions between the three factors that we have take into account. The results highlight the importance of making an experimental design where all of these parameters are adjusted for each problem.
	
	The proposed CLM has improved the performance of the deep network compared to the model that uses the softmax function and the models proposed in previous works. Also, it reduces the chance that the model gets stuck when training with some parameter configurations. So, the most significant improvements of these link functions are the performance increase, the reduction of the number of parameters configurations that should be tried to find the best one and the prevention of the over-fitting and the stagnation.

	
	% if have a single appendix:
	%\appendix[Proof of the Zonklar Equations]
	% or
	%\appendix  % for no appendix heading
	% do not use \section anymore after \appendix, only \section*
	% is possibly needed
	
	% use appendices with more than one appendix
	% then use \section to start each appendix
	% you must declare a \section before using any
	% \subsection or using \label (\appendices by itself
	% starts a section numbered zero.)
	%
	
	

	
	
	% Can use something like this to put references on a page
	% by themselves when using endfloat and the captionsoff option.
	\ifCLASSOPTIONcaptionsoff
	\newpage
	\fi
	
	% trigger a \newpage just before the given reference
	% number - used to balance the columns on the last page
	% adjust value as needed - may need to be readjusted if
	% the document is modified later
	%\IEEEtriggeratref{8}
	% The "triggered" command can be changed if desired:
	%\IEEEtriggercmd{\enlargethispage{-5in}}
	
	% references section
	
	% can use a bibliography generated by BibTeX as a .bbl file
	% BibTeX documentation can be easily obtained at:
	% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
	% The IEEEtran BibTeX style support page is at:
	% http://www.michaelshell.org/tex/ieeetran/bibtex/
	\bibliographystyle{IEEEtran}
	% argument is your BibTeX string definitions and bibliography database(s)
	\bibliography{main.bib}
	%
	% <OR> manually copy in the resultant .bbl file
	% set second argument of \begin to the number of references
	% (used to reserve space for the reference number labels box)

	%\printbibliography
		

	
	
	
	% You can push biographies down or up by placing
	% a \vfill before or after them. The appropriate
	% use of \vfill depends on what kind of text is
	% on the last page and whether or not the columns
	% are being equalized.
	
	%\vfill
	
	% Can be used to pull up biographies so that the bottom of the last one
	% is flush with the other column.
	%\enlargethispage{-5in}
	
	
	
	% that's all folks
\end{document}


