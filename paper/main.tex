% !TeX spellcheck = en_UK
\documentclass[10pt, a4paper, titlepage, twocolumn]{article}

\input{preambulo.tex}

\setlength{\columnsep}{0.7cm}

\begin{document}
	\fontfamily{lmr}\selectfont
	
	%\begin{multicols}{2}
		
	\section{Introduction}
	...
		
	\section{Related work}
	...

	
	\section{Experiments}
	\subsection{Data}
	We make use of two ordinal datasets appropriate for deep neural networks:
	
	\begin{itemize}
		\item \textit{Diabetic Retinopathy (DR)}\footnote{https://www.kaggle.com/c/diabetic-retinopathy-detection/data}. DR is a dataset consisting of extremely high-resolution fundus image data. The training set consists of $17563$ pairs of images (where a  pair consists of a left and right eye image corresponding to a patient). In this dataset, we try and predict from five levels of diabetic retinopathy: no DR ($25810$ images), mild DR ($2443$ images), moderate DR ($5292$ images), severe DR ($873$ images), or proliferative DR ($708$ images). The images are taken in variable conditions: by different cameras, illumination conditions and resolutions. These images come from the EyePACS dataset that was used in a Diabetic Retinopathy Detection competition that was hosted on the Kaggle platform. Also, this dataset was used in later works \cite{de2018weighted}\cite{nebot2016diabetic} and ordinal techniques (such as an ordinal cost function) were applied in order to achieve better performance. A validation set is set aside, consisting of $10\%$ of the patients in the training set. The images are resized to 256 by 256 pixels. Data augmentation techniques are applied to achieve a higher number of samples.
		
		\item \textit{Adience}\footnote{http://www.openu.ac.il/home/hassner/Adience/data.html}. This dataset consists of $26580$ faces belonging to $2284$ subjects. We use the form of the dataset where faces have been pre-cropped and aligned. The dataset was preprocessed, using the methods described in a previous work \cite{beckham2017unimodal}, so that the images are 256px in width and height, and pixels values follow a $(0;1)$ normal distribution. The original dataset is split into five cross-validation folds. The training set consists of merging the first four folds which comprise a total of $15554$ images. From this, $10\%$ of the images are held out as part of a validation set. The last fold is used as test set.
	\end{itemize}
	
	\subsection{The model}
	A convolutional neural network (CNN) has been used for both datasets. The architecture of this CNN is presented in the Table \ref{table:CNNArchitecture}.
	
	\begin{table}[htbp]
		\centering
		\begin{tabular}{|l|c|}
			\hline
			\textbf{Layer} & \textbf{Output shape}\\
			\hline
			Conv\_32\_3x3 & 254x254x32\\\hline
			Conv\_32\_3x3 & 252x252x32\\\hline
			MaxPool\_2x2 & 126x126x32\\\hline
			
			Conv\_64\_3x3 & 124x124x64\\\hline
			Conv\_64\_3x3 & 122x122x64\\\hline
			MaxPool\_2x2 & 61x61x64\\\hline
			
			Conv\_128\_3x3 & 59x59x128\\\hline
			Conv\_128\_3x3 & 57x57x128\\\hline
			MaxPool\_2x2 & 28x28x128\\\hline
			
			Conv\_128\_3x3 & 26x26x128\\\hline
			Conv\_128\_3x3 & 24x24x128\\\hline
			MaxPool\_2x2 & 12x12x128\\\hline
			
			Conv\_128\_4x4 & 9x9x128\\\hline
			Dense\_1\_output & 1\\
			\hline
		\end{tabular}
		\caption{Description of the architecture used in the experiments. For convolutional layers, Conv\_N\_WxH, where N is the number of filters, W the filter width and H the filter height. Stride is 1 for every convolutional layer. For max pool layers, MaxPool\_SxS, where S is the pool size.}
		\label{table:CNNArchitecture}
	\end{table}
	
	Every convolutional layer is followed by an ELU activation layer \cite{clevert2015fast} and a batch normalization \cite{ioffe2015batch}. At the output, a Proportional Odds Model (POM) is used with different link functions \cite{agresti2010analysis}. The logit link function is commonly used within POM. In this paper, we are comparing other link functions like probit or complementary log-log with the logit link. These three types of links are explained below.
	
	\begin{itemize}
		\item \textit{Logit}. Logit link function is the most widely used function for Proportional Odds Models. These kind of models are also called Cummulative Logit Models. The logit link is shown in Eq. \ref{eq:logit}.
		\begin{equation}
		\small
		logit[P(Y_i \le j)] = \alpha_j + \beta'x_i,\quad j = 1, ..., c - 1
		\label{eq:logit}
		\end{equation}
		\item \textit{Probit}. Probit link function is the inverse of the standard normal cummulative distribution function (cdf). Its expression is shown in Eq. \ref{eq:probit}.	
		\begin{equation}
		\small
		\Phi^{-1}[P(Y \le j)] = \alpha_j + \beta'x,\quad j = 1, ..., c - 1
		\label{eq:probit}
		\end{equation}
		\item \textit{Complementary log-log}. Unlike logit and probit, complementary log-log function is not symmetric. With a continuous predictor $x$, for example, $P(Y \le j)$ approaches 0 at a different rate than it approaches 1. Complementary log-log expression is shown in Eq. \ref{eq:cloglog}.
		\begin{equation}
		\footnotesize
		log[-log[1 - P(Y \le j)]] = \alpha_j + \beta'x,\quad j = 1, ..., c - 1
		\label{eq:cloglog}
		\end{equation}
	\end{itemize}
	
	\subsection{Procedure}
	The model is optimized using a batch based first-order optimization algorithm called Adam \cite{kingma2014adam}. We study different initial learning rates in order to find the optimal one for each problem. We apply an exponential decay across training epochs to the initial learning rate.
	
	Quadratic Weighted Kappa Loss, that J. de la Torre described in previous work, is considered as loss function for this optimizer as it gives better performance for ordinal classification problems.
	
	Both datasets have been artificially equalised using data augmentation techniques \cite{van2001art}\cite{krizhevsky2012imagenet} based on image cropping and zooming, horizontal and vertical flipping, brightness adjustment and random rotations. In the case of Diabetic Retinopathy Detection, the epoch size has been fixed to $100000$ images per epoch. For the Adience dataset, epoch size is the number of images in the training set.
	
	The model is evaluated with Quadratic Weighted Kappa metric (QWK) \cite{ben2008comparison}. This evaluation measure gives a higher weight to the errors that are further from the correct class.
	
	\subsection{Parameters}
	Three different parameters have been considered: learning rate, batch size and link function for the final output layer.
	
	\begin{itemize}
		\item \textit{Learning rate}. Learning rate is one of the most critical hyper-parameters to tune for training deep neural networks. Optimal learning rate can vary depending on the dataset and the CNN architecture. Previous works have presented some techniques that adjust this parameter in order to achieve better performance \cite{smith2017cyclical}\cite{senior2013empirical}. Within this work, we have considered three different values for this parameter: $10^{-2}$, $10^{-3}$ and $10^{-4}$.
		\item \textit{Batch size}. Batch size is also an important parameter as it controls the number of weight updates that are made on every epoch. It can affect the training time and the model performance. In this paper, we have tried three separate batch sizes: $5$, $10$ and $15$.
		\item \textit{Link function}. Different link functions have been used for the POM at the last layer output: logit, probit and complementary log-log.
	\end{itemize}
	
	\section{Results}
	In this section, we present the results of the experiments. For each dataset, we show a table with the detailed experiments done training the model with each combination of parameters. Every parameter combination has been run five times. These tables show the average quadratic weighted kappa across these five executions for validation and test values.
	
	\subsection{Diabetic Retinopathy}
	Detailed results for the Diabetic Retinopathy dataset are presented in Table \ref{table:DRresults}.
	
	\begin{table}[htbp]
		\footnotesize
		\centering
		\begin{tabular}{ccc|cc}
			BS & LR & LF & $\kappa_{val}$ & $\kappa_{test}$\\\hline\addlinespace[0.05cm]
			$5$ & $10^{-2}$ & Logit & $0.44888$ & $0.4163$\\
			& & Probit & $0.46724$ & $0.45972$\\
			& & c log-log & $0.42854$ & $0.41448$\\
			& $10^{-3}$ & Logit & $0.56496$ & $0.55356$\\
			& & Probit & $0.57084$ & $0.56392$\\
			& & c log-log & $0.54796$ & $0.53436$\\
			& $10^{-4}$ & Logit & $0.52884$ & $0.52032$\\
			& & Probit & $0.53802$ & $0.52302$\\
			& & c log-log & $0.53824$ & $0.51974$\\
			$10$ & $10^{-2}$ & Logit & $0.5497$ & $0.53142$\\
			& & Probit & $0.52124$ & $0.50806$\\
			& & c log-log & $0.4276$ & $0.4227$\\
			& $10^{-3}$ & Logit & $0.58036$ & $0.57686$\\
			& & Probit & $0.56536$ & $0.5581$\\
			& & c log-log & $\textbf{0.5883}$ & $\textbf{0.5815}$\\
			& $10^{-4}$ & Logit & $0.53158$ & $0.5385$\\
			& & Probit & $0.5456$ & $0.54082$\\
			& & c log-log & $0.53928$ & $0.53742$\\
			$15$ & $10^{-2}$ & Logit & $0.55828$ & $0.55076$\\
			& & Probit & $0.54112$ & $0.5343$\\
			& & c log-log & $0.56676$ & $0.56428$\\
			& $10^{-3}$ & Logit & $0.55748$ & $0.55106$\\
			& & Probit & $0.58182$ & $0.57894$\\
			& & c log-log & $0.5634$ & $0.55938$\\
			& $10^{-4}$ & Logit & $0.54686$ & $0.54342$\\
			& & Probit & $0.53258$ & $0.53278$\\
			& & c log-log & $0.539$ & $0.53828$
		\end{tabular}
		\caption{Diabetic Retinopathy results. BS stands for Batch Size, LR for Learning Rate and LF for link function.}
		\label{table:DRresults}
	\end{table}
	
	\subsection{Adience}
	Results for the experiments made with the Adience dataset are shown in Table \ref{table:Adienceresults}.
	
	\begin{table}[htbp]
		\footnotesize
		\centering
		\begin{tabular}{ccc|cc}
			BS & LR & LF & $\kappa_{val}$ & $\kappa_{test}$\\\hline\addlinespace[0.05cm]
			$5$ & $10^{-2}$ & Logit & $0.72072$ & $0.68408$\\
			& & Probit & $0.77608$ & $0.72574$\\
			& & c log-log & $0.7543$ & $0.70742$\\
			& $10^{-3}$ & Logit & $0.7146$ & $0.67868$\\
			& & Probit & $0.81906$ & $0.77154$\\
			& & c log-log & $0.57196$ & $0.55608$\\
			& $10^{-4}$ & Logit & $0.54432$ & $0.51934$\\
			& & Probit & $0.5436$ & $0.52422$\\
			& & c log-log & $0.54802$ & $0.52484$\\
			$10$ & $10^{-2}$ & Logit & $0.78872$ & $0.72994$\\
			& & Probit & $0.79332$ & $0.7475$\\
			& & c log-log & $0.78004$ & $0.7326$\\
			& $10^{-3}$ & Logit & $0.49686$ & $0.47112$\\
			& & Probit & $0.59778$ & $0.58476$\\
			& & c log-log & $0.52016$ & $0.49968$\\
			& $10^{-4}$ & Logit & $0.60972$ & $0.56814$\\
			& & Probit & $0.54638$ & $0.52632$\\
			& & c log-log & $0.75428$ & $0.52094$\\
			$15$ & $10^{-2}$ & Logit & $0.79104$ & $0.74388$\\
			& & Probit & $\textbf{0.84254}$ & $\textbf{0.78666}$\\
			& & c log-log & $0.78802$ & $0.74624$\\
			& $10^{-3}$ & Logit & $0.56946$ & $0.52978$\\
			& & Probit & $0.70262$ & $0.65416$\\
			& & c log-log & $0.49242$ & $0.4732$\\
			& $10^{-4}$ & Logit & $0.55202$ & $0.52484$\\
			& & Probit & $0.54288$ & $0.5174$\\
			& & c log-log & $0.53826$ & $0.52542$
		\end{tabular}
		\caption{Adience results. BS stands for Batch Size, LR for Learning Rate and LF for link function.}
		\label{table:Adienceresults}
	\end{table}
	
	\subsection{Statistical analysis}
	The significance and relative importance of the parameters concerning the results obtained, as well as suitable values for each, were obtained using ANalysis Of the VAriance (ANOVA)
	
	The ANalysis Of the VAriance (ANOVA) \cite{miller1997beyond} is one of the most widely used statistical techniques. ANOVA is essentially a method of analysing the variance to which a response is subject into its various components, corresponding to the sources of variation which can be identified.
	
	ANOVA, in this case, examines the effects of three quantitative variables (termed factors) on one quantitative response. Considered factors are the link function, which can be logit, probit and complementary log-log, the learning rate for the Adam optimisation algorithm ($10^{-2}$, $10^{-3}$ and $10^{-4}$) and the batch size (5, 10 and 15).
	
	Following the setup of the previous study, we performed an ANOVA III analysis and multiple comparison tests. The tests show that there is no batch size whose results are significatively better than the results of all other batch sizes. This does not mean that these differences could not exist for specific numbers of samples per batch. So, in order to determine for each type of function whether a batch size is better than the others, we have performed an ANOVA I analysis - where the only factor is the batch size - and multiple comparison tests.
	
	We denote by $W_{i,j,k,l}(i=1, ..., 3; j = 1, ..., 3; k = 1, ..., 3)$ the value observed when the first factor is at the i-th level, the second at the j-th level and the third at k-th level. We assume that the two factors do not act independently and therefore there exists an interaction between them. In this case, the observations fit Eq. \ref{eq:anova3}.
	
	\begin{equation}
	\begin{aligned}
	\footnotesize
	W_{i,j,k,l} = & \mu + L_i + P_j + B_k + LP_{i,j} + LB_{i,k} \\& + PB_{j,k} + LPB_{i,j,k} + \epsilon_{i,j,k,l}
	\label{eq:anova3}
	\end{aligned}
	\end{equation}
	
	where $\mu$ is the fixed effect that is common to all the populations; $L_i$ is the effect associated with the \textit{i-th} level of the link factor (logit, probit, complementary log-log); $P_j$ is the effect associated with the \textit{j-th} level of the parameter factor ($10^{-4}$, $10^{-3}$, $10^{-2}$) and $B_k$ is the effect associated with the \textit{k-th} level of the size batch factor (5, 10, 15). The term $LP_{i,j}$ denotes the joint effect of the presence of level $i$ of the first factor and level $j$ of the second one; this, therefore, is denominated the interaction term between $L$ and $P$ factors. The same interaction effect is appreciated on $LB_{i,k}$, $PB_{j,k}$ and $LPB_{i,j,k}$. The term $\epsilon_{i,j,k,l}$ is the influence on the result of everything that could not be assigned or of random factors. $W_{i,j,k,l}$ is the quadratic weighted kappa measure, the response variable used to perform the statistical analysis.
	
	We consider some hypotheses testing where the null hypothesis is proposed that each term of the above equation is independent of the levels involved. The hypotheses for the levels the L factor are
	\[
		H_0 \equiv L_1 = L_2 = L_3
	\]
	\[
		H_1 \equiv \text{ some } L_i \text{ is different}
	\]
	
	The same hypotheses are made for the other factors. In this way, we test in the null hypothesis that all of the population means are equal against an alternative hypothesis that there is at least one mean that is not equal to the others.
	
	The hypothesis associated with the interaction between L and P is
	\[
		H_0 \equiv LP_{i,j} = 0 \quad \forall i,j
	\]
	\[
		H_1 \equiv \exists LP_{i,j} \ne 0
	\]
	
	Similar hypotheses can be assumed for the interaction between the other factors.
	
	The analysis of variance table containing the sum of squares, degrees of freedom, mean square, test statistics and significance level represents the initial study in a compact form.
	
	\begin{table}[htbp]
		\centering
		\small
		\begin{tabular}{l|ccccc}
			\multicolumn{6}{c}{Response variable WK}\\\hline
			Source & S.S. & D.F. & M.S. & F-ratio & Sig.\\\hline
			Model & $37.860$ & $9$ & $4.207$ & $1562.840$ & $.000$\\
			$L$ factor & $.057$ & $2$ & $.029$ & $10.646$ & $.000$\\
			$P$ factor & $.121$ & $2$ & $.060$ & $22.468$ & $.000$\\
			$LP$ factors & $.057$ & $4$ & $.014$ & $5.261$ & $.001$\\
			Error & $.339$ & $126$ & $.003$ & & \\
			Total & $38.199$ & $135$ & & & 
		\end{tabular}
		\caption{ANOVA III for the analysis of the main factors in the design of a Convolutional Ordinal Neural Network for the Retinopathy dataset.}
		\label{table:ANOVARetinopathy}
	\end{table}
	
	The results of the ANOVA III test for the Diabetic Retinopathy dataset are resumed in Table \ref{table:ANOVARetinopathy}. There are significative differences in the means depending on the link function and also depending on the learning rate for $\alpha=0.05$. Moreover, an interaction between the link function and the learning rate can be recognised ($\text{p-value} = 0.001$).
	
	As Table \ref{table:ANOVARetinopathy} showed that there exist significative differences between the means, we are now analysing those differences. A post-hoc multiple comparison test has been performed on the average WK obtained. An HSD Tukey's test was made under the null hypothesis that the variance of the error of the dependent variable is the same between groups. The best link function is the complementary log-log, though it doesn't have significative differences with the probit function ($\alpha=0.05$). There are differences between the complementary log-log and the probit function concerning the logit function.
	
	The results have been obtained following a similar methodology with the different values of the learning parameter including the HSD Tukey's test and the learning parameter ranking. Significative differences can be observed in the mean values for $\eta = 10^{-3}$ with respect to the other values of the parameter ($\alpha=0.05$). Also, there are differences for $\eta = 10^{-4}$ concerning $\eta = 10^{-2}$.
	
	\begin{table}[htbp]
		\centering
		\small
		\begin{tabular}{l|ccccc}
			\multicolumn{6}{c}{Response variable WK}\\\hline
			Source & S.S. & D.F. & M.S. & F-ratio & Sig.\\\hline
			Model & $52.328$ & $15$ & $3.489$ & $1426.170$ & $.000$\\
			$L$ factor & $.027$ & $2$ & $.014$ & $5.582$ & $.005$\\
			$P$ factor & $1.031$ & $2$ & $.516$ & $210.809$ & $.000$\\
			$B$ factor & $.089$ & $2$ & $.045$ & $18.253$ & $.000$\\
			$LP$ factors & $.183$ & $4$ & $.046$ & $18.695$ & $.000$\\
			$PB$ factors & $.124$ & $4$ & $.031$ & $12.695$ & $.000$\\
			Error & $.294$ & $120$ & $.002$ & & \\
			Total & $52.622$ & $135$ & $3.489$ & & 
		\end{tabular}
		\caption{ANOVA III for the analysis of the main factors in the design of a Convolutional Ordinal Neural Network for the Adience dataset.}
		\label{table:ANOVAAdience}
	\end{table}
	
	The results of the ANOVA III test for the Adience dataset are shown in Table \ref{table:ANOVAAdience}. The interactions where there are not significative differences have been omitted. First, the factor associated with the link function is analysed. The differences between the means are significative ($\text{p-value} = 0.005$). Secondly, there are significative differences concerning the learning rate factor too ($\text{p-value} = 0.000$). Also, there are differences for the means for the batch size factor. Significative interactions exist between two pairs of factors: link function and learning rate, and learning rate and batch size ($\text{p-value = 0.000}$ for both cases).
	
	A post-hoc multiple comparison test has been performed on the average WK obtained for this dataset too. Under the null hypothesis that the variance of the error of the dependent variable is the same between groups, an HSD Tukey's test has been done. The results showed that the best link function is the logit one, in this case, with $0.632438$ as mean WK value. However, there are not significative differences between this function and the complementary log-log ($\text{p-value}=0.110$). It has differences with the probit function though ($\text{p-value} = 0.003$). The complementary log-log link reported the best results after the logit function, having a mean WK of $0.611287$. The complementary log-log function doesn't show significative differences with the probit function.
	The results have been obtained following a similar methodology with the different values of the learning parameter including the HSD Tukey's test and the learning parameter ranking. The test reported significative differences between $\eta=10^{-2}$ (mean value $0.733784$) and the rest of values of this parameter ($\alpha=0.005$), being this one the best value for this factor. The best value after $10^{-2}$ is $10^{-3}$, which have a mean WK value of $0.579889$. Lastly, the best value for the batch size parameter is $10$ (mean value $0.648700$) as it has significative differences with the rest of values. The second best value is $5$ (mean $0.605533$), but it has no significative differences with $15$ ($\text{p-value}= 0.194$).
	
	\section{Conclusions}
	In this section, the conclusions obtained from this work are exposed. The first thing that we have noticed is that the optimal values for the different parameters considered are problem-dependant. 
	
	The complementary log-log function has the best average performance across both datasets. It offers the best results in Diabetic Retinopathy dataset and the second best result in Adience dataset. These results provide an opportunity for exploring new generalised link functions.
	
	The best value for the learning rate parameter for  Diabetic Retinopathy dataset is $\eta = 10^{-3}$ while this value is the second best option for Adience dataset. It can be considered a good value for this parameter when training the model with new datasets.
	
	Both datasets have obtained the best performance with batch size 10.
	
	Also, the statistical tests reported that there are interactions between some parameters like the link function and the learning rate, for Diabetic Retinopathy, or those factors in addition to learning rate and batch size, for Adience dataset.
	
	
	\printbibliography
	
	%\end{multicols}
\end{document}